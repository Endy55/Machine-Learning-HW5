---
title: "Classification"
author: "Endy Zarate"
date: "02/24/2025"

---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](https://raw.githubusercontent.com/cd-public/D505/refs/heads/master/hws/src/classify.qmd) hosted on GitHub pages.

# 0. Quarto Type-setting

- This document is rendered with Quarto, and configured to embed an images using the `embed-resources` option in the header.
- If you wish to use a similar header, here's is the format specification for this document:

```email
format: 
  html:
    embed-resources: true
```

# 1. Setup

**Step Up Code:**

```{r}
sh <- suppressPackageStartupMessages
sh(library(tidyverse))
sh(library(caret))
sh(library(naivebayes))
sh(library(tidytext))
sh(library(SnowballC))
sh(library(pROC))      
sh(library(glmnet))
data(stop_words)
wine <- readRDS(gzcon(url("https://github.com/cd-public/D505/raw/master/dat/pinot.rds")))
```

# 2. Logistic Concepts

Why do we call it Logistic Regression even though we are using the technique for classification?

> <span style="color:red;font-weight:bold">TODO</span>: *The logistic regression returns a probability not a classification. That log is used as a cutoff point in a data model in order to determine a class. The regression is still occurring but it is used a tool to classify.*

# 3. Modeling

We train a logistic regression algorithm to classify a whether a wine comes from Marlborough using:

1. An 80-20 train-test split.
2. Three features engineered from the description
3. 5-fold cross validation.

We report Kappa after using the model to predict provinces in the holdout sample.

```{r}
# TODO
names(wine)[names(wine) == 'id'] = 'id'
desc_to_words = function(df, omits) { 
  df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% # get rid of stop words
    filter(!(word %in% omits))
}

words = desc_to_words(wine, c("wine","pinot","vineyard"))

words_to_stems = function(df) { 
  df %>%
    mutate(word = wordStem(word))
}

stems = words_to_stems(words)

filter_by_count = function(df, j) { 
  df %>%
    count(id, word) %>% 
    group_by(id) %>% mutate(exists = (n>0)) %>% ungroup %>% 
    group_by(word) %>% 
    mutate(total = sum(n)) %>% 
    filter(total > j)
}

pivoter = function(words, df) {
  words %>%
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% 
    drop_na() %>% 
    select(-id)
}

wine_words = function(df, j, stem) { 

  words <- desc_to_words(df, c("wine","pinot","vineyard"))
  
  if (stem) {
    words <- words_to_stems(words)
  }
  
  words <- filter_by_count(words, j)

  pivoter(words, df)
}
```
```{r}
winewhitehouse = wine_words(wine, 1000, T) %>% 
           mutate(marlborough = as.factor(province == "Marlborough"),
                  casablanca = as.factor(province == "Casablanca_Valley"),
                  burgundy = as.factor(province == "Burgundy")) %>%
           select(-province)
wine_index = createDataPartition(winewhitehouse$marlborough, p = 0.80, list = FALSE)
train = winewhitehouse[wine_index, ]
test = winewhitehouse[-wine_index, ]
```

```{r}
control = trainControl(method = "cv", number = 5)

fit = train(marlborough ~ .,
             data = train, 
             trControl = control,
             method = "glm",
             family = "binomial")

fit
```


# 4. Binary vs Other Classification

What is the difference between determining some form of classification through logistic regression versus methods like $K$-NN and Naive Bayes which performed classifications.

> <span style="color:red;font-weight:bold">TODO</span>: *The K-NN and Naive Bayes method are multi-class supervised classifications while Logistics regressions is a binary surpervised classification. Logistic regression calculates the relationship between an input and a class while Naive Bayes and K-NN is more broad and takes into account several classes.*


# 5. ROC Curves

We can display an ROC for the model to explain your model's quality.

```{r}
# You can find a tutorial on ROC curves here: https://towardsdatascience.com/understanding-the-roc-curve-and-auc-dd4f9a192ecb/

prob = predict(fit, newdata = test, type = "prob")[,2]
myRoc = roc(test$marlborough, prob)
plot(myRoc)
auc(myRoc)
```

> <span style="color:red;font-weight:bold">TODO</span>: *The ROC curve show us how well our logistic regression model performed. In this graph, there were an overwhelming amount of true positives or accurate predictions. The area under the curve number is about 0.95 which is close to 1. The model is a great classifier for wines from the providence of Marlborough.*